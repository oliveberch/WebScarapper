{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scrapper\n",
    "\n",
    "## Overview\n",
    "This project aims to scrape email addresses from a specified website and store them in a CSV file.\n",
    "\n",
    "## Usage Instructions\n",
    "1. Install the required libraries:\n",
    "   ```\n",
    "   pip install requests beautifulsoup4\n",
    "   ```\n",
    "2. Open the web_scraping_project.ipynb file in a Jupyter Notebook environment.\n",
    "\n",
    "3. Customize the target_url variable with the URL of the website you want to scrape.\n",
    "\n",
    "4. Run the cells sequentially.\n",
    "\n",
    "5. The scraped email addresses will be stored in a CSV file named scraped_emails.csv in the same directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to scrape emails from a website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_emails(url):\n",
    "    \"\"\"\n",
    "    Scrapes email addresses from a given website.\n",
    "    \n",
    "    Parameters:\n",
    "    - url (str): The URL of the website to scrape.\n",
    "    \n",
    "    Returns:\n",
    "    - list: A list of scraped email addresses.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Establish connection and get HTML content\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract email addresses (adjust this based on the website structure)\n",
    "        emails = [email.get('href') for email in soup.find_all('a', href=lambda x: x and 'mailto:' in x)]\n",
    "\n",
    "        return emails\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to write emails to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(emails, filename):\n",
    "    \"\"\"\n",
    "    Writes a list of emails to a CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    - emails (list): A list of email addresses.\n",
    "    - filename (str): The name of the CSV file to write.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filename, 'w', newline='') as csvfile:\n",
    "            csv_writer = csv.writer(csvfile)\n",
    "            csv_writer.writerow(['Email'])\n",
    "\n",
    "            for email in emails:\n",
    "                csv_writer.writerow([email])\n",
    "\n",
    "        print(f\"Data written to {filename}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to CSV: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Main execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Step 1: Specify the URL to scrape\n",
    "    target_url = 'https://thenytimes.com'\n",
    "\n",
    "    # Step 2: Scrape emails\n",
    "    scraped_emails = scrape_emails(target_url)\n",
    "\n",
    "    # Step 3: Specify the CSV filename\n",
    "    output_csv = 'scraped_emails.csv'\n",
    "\n",
    "    # Step 4: Write emails to CSV\n",
    "    write_to_csv(scraped_emails, output_csv)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
